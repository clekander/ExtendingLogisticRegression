{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0d336ca",
   "metadata": {},
   "source": [
    "# Lab Assignment 3: Extending Logistic Regression\n",
    "\n",
    "Gabs DiLiegro, London Kasper, Carys LeKander\n",
    "\n",
    "# 1. Preparation and Overview\n",
    "\n",
    "Our dataset is centered around housing in neighborhoods around Melbourne, Australia. Our classification task is based on the prices of the housing. We have separated our data into three categories (less than 500,000, 500,000 - 1,000,000, and greater than 1,000,000). Being able to predict what range the housing cost lies in could help buyers and sellers in the housing market as people looking to buy a house could use this model to see what factors drive prices up and be able to stay in their price range and sellers could get an accurate estimate of how much their home is worth. \n",
    "\n",
    "The cost of misclasification is mainly misleading users about the value of a home which could cost the user money. People looking for houses could think a house was in their price range when it actually is too expensive or they may offer more money for a house than it is  worth. Similarly, peoople selling their house may list their house for less or more than its true value. \n",
    "\n",
    "The accuracy of classification is the main result we will look at to determine our success. We believe that the accuracy of our model would need to be a high percentage to be accepted by users because there are many algorithms about predicting housing prices that currently exist with high accuracy. When looking online, we found multiple tools built to forecast prices of homes (ex. VeroFORECAST) and research papers about machine learning for predicting housing costs (ex. one we found claims 86% accuracy). \n",
    "\n",
    "Dataset: https://www.kaggle.com/datasets/dansbecker/melbourne-housing-snapshot\n",
    "\n",
    "******************************************************\n",
    "\n",
    "VeroFORCAST: https://www.veros.com/solutions/home-price-trends-and-forecast/veroforecast\n",
    "\n",
    "Reasearch paper example: https://www.ijert.org/comparison-of-machine-learning-algorithms-for-house-price-prediction-using-real-time-data\n",
    "\n",
    "\n",
    "## Define and Prepare Class Variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79f05bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13580 entries, 0 to 13579\n",
      "Data columns (total 21 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Suburb         13580 non-null  object \n",
      " 1   Address        13580 non-null  object \n",
      " 2   Rooms          13580 non-null  int64  \n",
      " 3   Type           13580 non-null  object \n",
      " 4   Price          13580 non-null  float64\n",
      " 5   Method         13580 non-null  object \n",
      " 6   SellerG        13580 non-null  object \n",
      " 7   Date           13580 non-null  object \n",
      " 8   Distance       13580 non-null  float64\n",
      " 9   Postcode       13580 non-null  float64\n",
      " 10  Bedroom2       13580 non-null  float64\n",
      " 11  Bathroom       13580 non-null  float64\n",
      " 12  Car            13518 non-null  float64\n",
      " 13  Landsize       13580 non-null  float64\n",
      " 14  BuildingArea   7130 non-null   float64\n",
      " 15  YearBuilt      8205 non-null   float64\n",
      " 16  CouncilArea    12211 non-null  object \n",
      " 17  Lattitude      13580 non-null  float64\n",
      " 18  Longtitude     13580 non-null  float64\n",
      " 19  Regionname     13580 non-null  object \n",
      " 20  Propertycount  13580 non-null  float64\n",
      "dtypes: float64(12), int64(1), object(8)\n",
      "memory usage: 2.2+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('melb_data.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e824fd43",
   "metadata": {},
   "source": [
    "In Assingment 1, we explained why we decided to remove certain attributes. We have listed descriptions of the attributes used and removed below.\n",
    "\n",
    "### Attributes used:\n",
    "- Suburb: the name of the suburb that each property is in. (String object)\n",
    "- Rooms: the total number of rooms for each property. (int64)\n",
    "- Type: the type of property. ( h = house; u = unit, duplex; t = townhouse)\n",
    "- Price: listing price in Australian dollars (float64)\n",
    "- Distance: the distance from the property to the Melbourne central business district AKA CBD (float64)\n",
    "- Postcode: zipcode the property falls within (float64) \n",
    "- Bathroom: the number of bathrooms (float64)\n",
    "- Car: the number of parking spots (float64)\n",
    "- Landsize: the size of the land in meters (float64)\n",
    "- Regionname: general region of the property (String object)\n",
    "\n",
    "### Attributes removed and why:\n",
    "- Address: the address of the property (String object)\n",
    "- Method: way the property was listed\n",
    "     - We aren't considering data about how the house was sold, just the features of the house itself, which is why we also excluded SellerG and Date\n",
    "- SellerG: name of the real estate agent listing the property (String object)\n",
    "- Date: sale date in mm/dd/yyyy (float64)\n",
    "- Bedroom2: the number of bedrooms (float64: scraped from a different source)\n",
    "    - Rooms and Bedroom2 contain the same information collected from different sources. We have seen that these features are very strongly correlated and have very similar data, so we are excluding Bedroom2 for simplicity.\n",
    "- Propertycount: number of properties in the same suburb (float64)\n",
    "- BuildingArea: area of the building in meters\n",
    "    - Since ~47% of the dataset is missing this we decided to remove it\n",
    "- YearBuilt: year the house was built (float64)\n",
    "    - Although we think YearBuilt could have useful infomation for our model, ~40% of the data is missing so we have decided to remove it\n",
    "- CouncilArea: the governing council for the area (String object)\n",
    "    - The CouncilArea stopped being recorded after a certain date. Therefore we decided to remove this attribute as we did not want it to skew our set\n",
    "- Latitude: lattitude of property (float64)\n",
    "- Longitude: longtitude of property (float64)\n",
    "    - Niether Longitude or Latitude supply us with more useful information than other data we have collected about the area of the houses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f751bdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing irrelevant columns \n",
    "df = df.drop(['Address','Method','SellerG', 'Date','Bedroom2', 'Propertycount','BuildingArea', 'YearBuilt','CouncilArea','Lattitude','Longtitude'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a16d5a",
   "metadata": {},
   "source": [
    "Our only column remaining with missing data is the car spots. There are only 62 missing data points of 13,580."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70cfad11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    13518.000000\n",
       "mean         1.610075\n",
       "std          0.962634\n",
       "min          0.000000\n",
       "25%          1.000000\n",
       "50%          2.000000\n",
       "75%          2.000000\n",
       "max         10.000000\n",
       "Name: Car, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Car.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2285a9f",
   "metadata": {},
   "source": [
    "Since there are so few missing points and the interquartile range is small, we used the median (2 spots) to fill the missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fd2a4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13580 entries, 0 to 13579\n",
      "Data columns (total 10 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Suburb      13580 non-null  object \n",
      " 1   Rooms       13580 non-null  int64  \n",
      " 2   Type        13580 non-null  object \n",
      " 3   Price       13580 non-null  float64\n",
      " 4   Distance    13580 non-null  float64\n",
      " 5   Postcode    13580 non-null  float64\n",
      " 6   Bathroom    13580 non-null  float64\n",
      " 7   Car         13580 non-null  float64\n",
      " 8   Landsize    13580 non-null  float64\n",
      " 9   Regionname  13580 non-null  object \n",
      "dtypes: float64(6), int64(1), object(3)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "#fill in missing numeric values with median for Car\n",
    "df.Car = df.Car.fillna(df.Car.median())\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a3d4db",
   "metadata": {},
   "source": [
    "Next, we want to normalize our numeric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09fc6204",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "cols_to_norm = ['Rooms','Distance','Bathroom','Car','Landsize']\n",
    "df[cols_to_norm] = MinMaxScaler().fit_transform(df[cols_to_norm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4858dfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Suburb</th>\n",
       "      <th>Rooms</th>\n",
       "      <th>Type</th>\n",
       "      <th>Price</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Postcode</th>\n",
       "      <th>Bathroom</th>\n",
       "      <th>Car</th>\n",
       "      <th>Landsize</th>\n",
       "      <th>Regionname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abbotsford</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>h</td>\n",
       "      <td>1480000.0</td>\n",
       "      <td>0.051975</td>\n",
       "      <td>3067.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>Northern Metropolitan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abbotsford</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>h</td>\n",
       "      <td>1035000.0</td>\n",
       "      <td>0.051975</td>\n",
       "      <td>3067.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000360</td>\n",
       "      <td>Northern Metropolitan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abbotsford</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>h</td>\n",
       "      <td>1465000.0</td>\n",
       "      <td>0.051975</td>\n",
       "      <td>3067.0</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>Northern Metropolitan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abbotsford</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>h</td>\n",
       "      <td>850000.0</td>\n",
       "      <td>0.051975</td>\n",
       "      <td>3067.0</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>Northern Metropolitan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abbotsford</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>h</td>\n",
       "      <td>1600000.0</td>\n",
       "      <td>0.051975</td>\n",
       "      <td>3067.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>Northern Metropolitan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Suburb     Rooms Type      Price  Distance  Postcode  Bathroom  Car  \\\n",
       "0  Abbotsford  0.111111    h  1480000.0  0.051975    3067.0     0.125  0.1   \n",
       "1  Abbotsford  0.111111    h  1035000.0  0.051975    3067.0     0.125  0.0   \n",
       "2  Abbotsford  0.222222    h  1465000.0  0.051975    3067.0     0.250  0.0   \n",
       "3  Abbotsford  0.222222    h   850000.0  0.051975    3067.0     0.250  0.1   \n",
       "4  Abbotsford  0.333333    h  1600000.0  0.051975    3067.0     0.125  0.2   \n",
       "\n",
       "   Landsize             Regionname  \n",
       "0  0.000466  Northern Metropolitan  \n",
       "1  0.000360  Northern Metropolitan  \n",
       "2  0.000309  Northern Metropolitan  \n",
       "3  0.000217  Northern Metropolitan  \n",
       "4  0.000277  Northern Metropolitan  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8aeb473",
   "metadata": {},
   "source": [
    "Then, we one-hot encode our categorical data.\n",
    "\n",
    "Type has 3 unqiue values ('h' for house, 'u' for unit, and 't' for townhouse) and Regionname has 8 unique regions. We one-hot encode those below and show the new variables we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c26c583c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13580 entries, 0 to 13579\n",
      "Data columns (total 19 columns):\n",
      " #   Column                                 Non-Null Count  Dtype  \n",
      "---  ------                                 --------------  -----  \n",
      " 0   Suburb                                 13580 non-null  object \n",
      " 1   Rooms                                  13580 non-null  float64\n",
      " 2   Price                                  13580 non-null  float64\n",
      " 3   Distance                               13580 non-null  float64\n",
      " 4   Postcode                               13580 non-null  float64\n",
      " 5   Bathroom                               13580 non-null  float64\n",
      " 6   Car                                    13580 non-null  float64\n",
      " 7   Landsize                               13580 non-null  float64\n",
      " 8   Type_h                                 13580 non-null  uint8  \n",
      " 9   Type_t                                 13580 non-null  uint8  \n",
      " 10  Type_u                                 13580 non-null  uint8  \n",
      " 11  Regionname_Eastern Metropolitan        13580 non-null  uint8  \n",
      " 12  Regionname_Eastern Victoria            13580 non-null  uint8  \n",
      " 13  Regionname_Northern Metropolitan       13580 non-null  uint8  \n",
      " 14  Regionname_Northern Victoria           13580 non-null  uint8  \n",
      " 15  Regionname_South-Eastern Metropolitan  13580 non-null  uint8  \n",
      " 16  Regionname_Southern Metropolitan       13580 non-null  uint8  \n",
      " 17  Regionname_Western Metropolitan        13580 non-null  uint8  \n",
      " 18  Regionname_Western Victoria            13580 non-null  uint8  \n",
      "dtypes: float64(7), object(1), uint8(11)\n",
      "memory usage: 994.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([df,pd.get_dummies(df['Type'], prefix='Type')],axis=1)\n",
    "df.drop(['Type'],axis=1, inplace=True)\n",
    "\n",
    "df = pd.concat([df,pd.get_dummies(df['Regionname'], prefix='Regionname')],axis=1)\n",
    "df.drop(['Regionname'],axis=1, inplace=True)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cd97c3",
   "metadata": {},
   "source": [
    "Postcode and Suburb have way more unique values. We one-hot encode them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "382cbea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13580 entries, 0 to 13579\n",
      "Columns: 529 entries, Rooms to Suburb_Yarraville\n",
      "dtypes: float64(6), uint8(523)\n",
      "memory usage: 7.4 MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([df,pd.get_dummies(df['Postcode'], prefix='Postcode')],axis=1)\n",
    "df.drop(['Postcode'],axis=1, inplace=True)\n",
    "\n",
    "df = pd.concat([df,pd.get_dummies(df['Suburb'], prefix='Suburb')],axis=1)\n",
    "df.drop(['Suburb'],axis=1, inplace=True)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6d318e",
   "metadata": {},
   "source": [
    "## Divide Data:\n",
    "\n",
    "Now getting ready to split our testing and training data, we define the bins for our price ranges and label those groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eda3d11f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rooms</th>\n",
       "      <th>Price</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Bathroom</th>\n",
       "      <th>Car</th>\n",
       "      <th>Landsize</th>\n",
       "      <th>Type_h</th>\n",
       "      <th>Type_t</th>\n",
       "      <th>Type_u</th>\n",
       "      <th>Regionname_Eastern Metropolitan</th>\n",
       "      <th>...</th>\n",
       "      <th>Suburb_Williamstown</th>\n",
       "      <th>Suburb_Williamstown North</th>\n",
       "      <th>Suburb_Windsor</th>\n",
       "      <th>Suburb_Wollert</th>\n",
       "      <th>Suburb_Wonga Park</th>\n",
       "      <th>Suburb_Wyndham Vale</th>\n",
       "      <th>Suburb_Yallambie</th>\n",
       "      <th>Suburb_Yarra Glen</th>\n",
       "      <th>Suburb_Yarraville</th>\n",
       "      <th>Price_Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.111111</td>\n",
       "      <td>1480000.0</td>\n",
       "      <td>0.051975</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.111111</td>\n",
       "      <td>1035000.0</td>\n",
       "      <td>0.051975</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000360</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.222222</td>\n",
       "      <td>1465000.0</td>\n",
       "      <td>0.051975</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.222222</td>\n",
       "      <td>850000.0</td>\n",
       "      <td>0.051975</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>1600000.0</td>\n",
       "      <td>0.051975</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 530 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Rooms      Price  Distance  Bathroom  Car  Landsize  Type_h  Type_t  \\\n",
       "0  0.111111  1480000.0  0.051975     0.125  0.1  0.000466       1       0   \n",
       "1  0.111111  1035000.0  0.051975     0.125  0.0  0.000360       1       0   \n",
       "2  0.222222  1465000.0  0.051975     0.250  0.0  0.000309       1       0   \n",
       "3  0.222222   850000.0  0.051975     0.250  0.1  0.000217       1       0   \n",
       "4  0.333333  1600000.0  0.051975     0.125  0.2  0.000277       1       0   \n",
       "\n",
       "   Type_u  Regionname_Eastern Metropolitan  ...  Suburb_Williamstown  \\\n",
       "0       0                                0  ...                    0   \n",
       "1       0                                0  ...                    0   \n",
       "2       0                                0  ...                    0   \n",
       "3       0                                0  ...                    0   \n",
       "4       0                                0  ...                    0   \n",
       "\n",
       "   Suburb_Williamstown North  Suburb_Windsor  Suburb_Wollert  \\\n",
       "0                          0               0               0   \n",
       "1                          0               0               0   \n",
       "2                          0               0               0   \n",
       "3                          0               0               0   \n",
       "4                          0               0               0   \n",
       "\n",
       "   Suburb_Wonga Park  Suburb_Wyndham Vale  Suburb_Yallambie  \\\n",
       "0                  0                    0                 0   \n",
       "1                  0                    0                 0   \n",
       "2                  0                    0                 0   \n",
       "3                  0                    0                 0   \n",
       "4                  0                    0                 0   \n",
       "\n",
       "   Suburb_Yarra Glen  Suburb_Yarraville  Price_Category  \n",
       "0                  0                  0               2  \n",
       "1                  0                  0               2  \n",
       "2                  0                  0               2  \n",
       "3                  0                  0               1  \n",
       "4                  0                  0               2  \n",
       "\n",
       "[5 rows x 530 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bins = [0, 500000, 1000000, 10000000]\n",
    "category = [0, 1, 2]\n",
    "df['Price_Category'] = pd.cut(df['Price'], bins, labels=category)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1026ff88",
   "metadata": {},
   "source": [
    "Next, we define our y that will be used when we test and train and remove the price and categorical price from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8062775d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    6240\n",
       "2    5743\n",
       "0    1597\n",
       "Name: Price_Category, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totals = df['Price_Category'].value_counts()\n",
    "y = df.Price_Category\n",
    "df.drop(['Price'],axis=1, inplace=True)\n",
    "df.drop(['Price_Category'],axis=1, inplace=True)\n",
    "totals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74921fe9",
   "metadata": {},
   "source": [
    "Even though there is significantly less data for the less than 500,000 range, we chose to keep the categories as is because we wanted the lowest range max not to be too expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15418eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, train_size=0.8, shuffle=True)\n",
    "y_train = y_train.values.tolist()\n",
    "y_test = y_test.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19466cae",
   "metadata": {},
   "source": [
    "The 80/20 train test split is good for our dataset because of its size. Even though our first range of houses is smaller, we always got over 1,000 instances in our training data for that category as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1838d02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1288\n",
      "4962\n",
      "4614\n"
     ]
    }
   ],
   "source": [
    "total_0 = y_train.count(0)\n",
    "total_1 = y_train.count(1)\n",
    "total_2 = y_train.count(2)\n",
    "\n",
    "print(total_0)\n",
    "print(total_1)\n",
    "print(total_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd9012a",
   "metadata": {},
   "source": [
    "# 2. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "332e84ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "from numpy.linalg import pinv\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression as SKLogisticRegression\n",
    "from scipy.optimize import fmin_bfgs # maybe the most common bfgs algorithm in the world\n",
    "from numpy import ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b913a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, C=0.001, regularization='none'):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        self.regularization = regularization\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "        \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    # convenience, private:\n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    # vectorized gradient calculation with regularization using L2 Norm\n",
    "    def _get_gradient(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        \n",
    "        if (self.regularization == 'L2'):\n",
    "            gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        elif(self.regularization == 'L1'):\n",
    "            gradient[1:] += (np.sign(self.w_[1:])) * self.C\n",
    "        elif(self.regularization == 'both'):\n",
    "            gradient[1:] += (np.sign(self.w_[1:])) * self.C + (-2 * self.w_[1:] * self.C)\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "    # public:\n",
    "    def predict_proba(self,X,add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) #return the actual prediction\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate \n",
    "            # add bacause maximizing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e542a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticLogisticRegression(BinaryLogisticRegression):\n",
    "\n",
    "    def _get_gradient(self,X,y):\n",
    "        idx = int(np.random.rand()*len(y)) # grab random instance\n",
    "        ydiff = y[idx]-self.predict_proba(X[idx],add_bias=False) # get y difference (now scalar)\n",
    "        gradient = X[idx] * ydiff[:,np.newaxis] # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        \n",
    "        if (self.regularization == 'L2'):\n",
    "            gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        elif(self.regularization == 'L1'):\n",
    "            gradient[1:] += (np.sign(self.w_[1:])) * self.C\n",
    "        elif(self.regularization == 'both'):\n",
    "            gradient[1:] += (np.sign(self.w_[1:])) * self.C + (-2 * self.w_[1:] * self.C)\n",
    "            \n",
    "        return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a681f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BFGSBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    \n",
    "    @staticmethod\n",
    "    def objective_gradient(w,X,y,C,regularization):\n",
    "        g = expit(X @ w)\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(w.shape)\n",
    "        \n",
    "        if (regularization == 'L2'):\n",
    "            gradient[1:] += -2 * w[1:] * C\n",
    "        elif(regularization == 'L1'):\n",
    "            gradient[1:] += (np.sign(w[1:])) * C\n",
    "        elif(regularization == 'both'):\n",
    "            gradient[1:] += ((np.sign(w[1:])) * C) + (-2 * w[1:] * C)\n",
    "        return -gradient\n",
    "    \n",
    "    @staticmethod\n",
    "    def objective_function(w,X,y,C,regularization):\n",
    "        g = expit(X @ w)\n",
    "        # invert this because scipy minimizes, but we derived all formulas for maximzing\n",
    "        return -np.sum(ma.log(g[y==1]))-np.sum(ma.log(1-g[y==0])) + C*sum(w**2) \n",
    "        #-np.sum(y*np.log(g)+(1-y)*np.log(1-g))\n",
    "    \n",
    "    # just overwrite fit function\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = fmin_bfgs(self.objective_function, # what to optimize\n",
    "                            np.zeros((num_features,1)), # starting point\n",
    "                            fprime=self.objective_gradient, # gradient function\n",
    "                            args=(Xb,y,self.C,self.regularization), # extra args for gradient and objective function\n",
    "                            gtol=1e-03, # stopping criteria for gradient, |v_k|\n",
    "                            maxiter=self.iters, # stopping criteria iterations\n",
    "                            disp=False)\n",
    "        \n",
    "        self.w_ = self.w_.reshape((num_features,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11f0e427",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassLogisticRegression:\n",
    "    def __init__(self, eta, iterations=20, \n",
    "                 C=0.0001, \n",
    "                 solver=BFGSBinaryLogisticRegression,\n",
    "                regularization='none'):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.C = C\n",
    "        self.solver = solver\n",
    "        self.classifiers_ = []\n",
    "        self.regularization = regularization\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.unique_ = np.sort(np.unique(y)) # get each unique class value\n",
    "        num_unique_classes = len(self.unique_)\n",
    "        self.classifiers_ = []\n",
    "        for i,yval in enumerate(self.unique_): # for each unique value\n",
    "            y_binary = np.array(y==yval).astype(int) # create a binary problem\n",
    "            # train the binary classifier for this class\n",
    "            \n",
    "            hblr = self.solver(eta=self.eta,iterations=self.iters,C=self.C, regularization = self.regularization)\n",
    "            hblr.fit(X,y_binary)\n",
    "\n",
    "            # add the trained classifier to the list\n",
    "            self.classifiers_.append(hblr)\n",
    "            \n",
    "        # save all the weights into one matrix, separate column for each class\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        probs = []\n",
    "        for hblr in self.classifiers_:\n",
    "            probs.append(hblr.predict_proba(X).reshape((len(X),1))) # get probability for each classifier\n",
    "        \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return np.argmax(self.predict_proba(X),axis=1) # take argmax along row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28349557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lr(C,solver,regularization):\n",
    "    lr = MultiClassLogisticRegression(eta=1, iterations=20, C=C, solver=solver, regularization=regularization)\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "    acc = accuracy_score(y_test,yhat)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a530072",
   "metadata": {},
   "outputs": [],
   "source": [
    "ste_001 = run_lr(.001,BinaryLogisticRegression,'L1')\n",
    "ste_01 = run_lr(.01,BinaryLogisticRegression,'L1')\n",
    "ste_1 = run_lr(.1,BinaryLogisticRegression,'L1')\n",
    "\n",
    "ste_001_L2 = run_lr(.001,BinaryLogisticRegression,'L2')\n",
    "ste_01_L2 = run_lr(.01,BinaryLogisticRegression,'L2')\n",
    "ste_1_L2 = run_lr(.1,BinaryLogisticRegression,'L2')\n",
    "\n",
    "ste_001_b = run_lr(.001,BinaryLogisticRegression,'both')\n",
    "ste_01_b = run_lr(.01,BinaryLogisticRegression,'both')\n",
    "ste_1_b = run_lr(.1,BinaryLogisticRegression,'both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ca3d17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sto_001 = run_lr(.001,StochasticLogisticRegression,'L1')\n",
    "sto_01 = run_lr(.01,StochasticLogisticRegression,'L1')\n",
    "sto_1 = run_lr(.1,StochasticLogisticRegression,'L1')\n",
    "\n",
    "sto_001_L2 = run_lr(.001,StochasticLogisticRegression,'L2')\n",
    "sto_01_L2 = run_lr(.01,StochasticLogisticRegression,'L2')\n",
    "sto_1_L2 = run_lr(.1,StochasticLogisticRegression,'L2')\n",
    "\n",
    "sto_001_b = run_lr(.001,StochasticLogisticRegression,'both')\n",
    "sto_01_b = run_lr(.01,StochasticLogisticRegression,'both')\n",
    "sto_1_b = run_lr(.1,StochasticLogisticRegression,'both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c48f801",
   "metadata": {},
   "outputs": [],
   "source": [
    "newt_001 = run_lr(.001,BFGSBinaryLogisticRegression,'L1')\n",
    "newt_01 = run_lr(.01,BFGSBinaryLogisticRegression,'L1')\n",
    "newt_1 = run_lr(.1,BFGSBinaryLogisticRegression,'L1')\n",
    "\n",
    "newt_001_L2 = run_lr(.001,BFGSBinaryLogisticRegression,'L2')\n",
    "newt_01_L2 = run_lr(.01,BFGSBinaryLogisticRegression,'L2')\n",
    "newt_1_L2 = run_lr(.1,BFGSBinaryLogisticRegression,'L2')\n",
    "\n",
    "newt_001_b = run_lr(.001,BFGSBinaryLogisticRegression,'both')\n",
    "newt_01_b = run_lr(.01,BFGSBinaryLogisticRegression,'both')\n",
    "newt_1_b = run_lr(.1,BFGSBinaryLogisticRegression,'both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9593c688",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_labels = ['C=.001','C=.01','C=.1']\n",
    "row_labels = ['Steepest Ascent','Stochastic','Quasi Newton']\n",
    "\n",
    "\n",
    "param_L1 = pd.DataFrame([[ste_001, ste_01 , ste_1],[sto_001, sto_01 ,sto_1],[newt_001, newt_01 ,newt_1]], columns=col_labels, index=row_labels)\n",
    "param_L2 = pd.DataFrame([[ste_001_L2, ste_01_L2 ,ste_1_L2],[sto_001_L2, sto_01_L2 ,sto_1_L2],[newt_001_L2, newt_01_L2 ,newt_1_L2]], columns=col_labels, index=row_labels)\n",
    "param_both = pd.DataFrame([[ste_001_b, ste_01_b ,ste_1_b],[sto_001_b, sto_01_b ,sto_1_b],[newt_001_b, newt_01_b ,newt_1_b]], columns=col_labels, index=row_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f63aeb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   C=.001     C=.01      C=.1\n",
      "Steepest Ascent  0.680412  0.730854  0.685567\n",
      "Stochastic       0.492636  0.585420  0.543078\n",
      "Quasi Newton     0.734904  0.662371  0.470545\n"
     ]
    }
   ],
   "source": [
    "print(param_L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6cf324d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   C=.001     C=.01      C=.1\n",
      "Steepest Ascent  0.666053  0.664212  0.662003\n",
      "Stochastic       0.485641  0.481222  0.576951\n",
      "Quasi Newton     0.741532  0.717968  0.643225\n"
     ]
    }
   ],
   "source": [
    "print(param_L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d218724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   C=.001     C=.01      C=.1\n",
      "Steepest Ascent  0.680412  0.726804  0.679676\n",
      "Stochastic       0.523564  0.492636  0.470545\n",
      "Quasi Newton     0.740795  0.708763  0.578792\n"
     ]
    }
   ],
   "source": [
    "print(param_both)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5489f270",
   "metadata": {},
   "source": [
    "Comparing our best logistic regressions to scikit learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1715649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of:  0.7308541973490427\n",
      "CPU times: total: 6.31 s\n",
      "Wall time: 4.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr = MultiClassLogisticRegression(eta=1, iterations=20, C=.01, solver=BinaryLogisticRegression, regularization='L1')\n",
    "lr.fit(X_train, y_train)\n",
    "yhat = lr.predict(X_test)\n",
    "print('Accuracy of: ',accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "23500da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of:  0.7415316642120766\n",
      "CPU times: total: 14.8 s\n",
      "Wall time: 9.89 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr = MultiClassLogisticRegression(eta=1, iterations=20, C=.001, solver=BFGSBinaryLogisticRegression, regularization='L2')\n",
    "lr.fit(X_train, y_train)\n",
    "yhat = lr.predict(X_test)\n",
    "print('Accuracy of: ',accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d034056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of:  0.7407952871870398\n",
      "CPU times: total: 13 s\n",
      "Wall time: 8.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr = MultiClassLogisticRegression(eta=1, iterations=20, C=.001, solver=BFGSBinaryLogisticRegression, regularization='both')\n",
    "lr.fit(X_train, y_train)\n",
    "yhat = lr.predict(X_test)\n",
    "print('Accuracy of: ',accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794b6ec7",
   "metadata": {},
   "source": [
    "We found three options above that worked well for our dataset. The two that worked best used the Quasi-Netwon (BFGS) where C=0.001. One used L2 regularizaton, while the other used both L1 and L2 regularization. For our model, the accuracy is our most important result. Comparing our accuracy to scikit-learn below we can see that we are not far off from theirs. The training time is significantly worse with our regressions though. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "78c9e9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of:  0.7971281296023565\n",
      "CPU times: total: 594 ms\n",
      "Wall time: 476 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#scikit learn\n",
    "lr_sk = SKLogisticRegression(solver='liblinear') # all params default\n",
    "\n",
    "lr_sk.fit(X_train,y_train)\n",
    "yhat = lr_sk.predict(X_test)\n",
    "print('Accuracy of: ',accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d26a318",
   "metadata": {},
   "source": [
    "# 3. Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f489d9",
   "metadata": {},
   "source": [
    "We would advise that scikit-learn be used for the implementation of logistic regression in a deployed machine learning model. Although our we can achieve on average around 74% accuracy with our best regressions, scikit-learn gets about 80%. We would advie scikit-learn, so that users have less misclassifications. \n",
    "\n",
    "Scikit-learn's CPU time is in the hundreds of ms range while ours are around 5-15 seconds. We do not think the training time is very important for our model as once it is deployed, it would not need to be retrained. \n",
    "\n",
    "*Who cares or do they care? Performance and how it impacts third party?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02799f81",
   "metadata": {},
   "source": [
    "# 4. Exceptional Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "033f67b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBFGSBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    \n",
    "    @staticmethod\n",
    "    def objective_gradient(w,X,y,C,regularization):\n",
    "        g = expit(X @ w)\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0)\n",
    "        gradient = gradient.reshape(w.shape)\n",
    "        \n",
    "        if (regularization == 'L2'):\n",
    "            gradient[1:] += -2 * w[1:] * C\n",
    "        elif(regularization == 'L1'):\n",
    "            gradient[1:] += (np.sign(w[1:])) * C\n",
    "        elif(regularization == 'both'):\n",
    "            gradient[1:] += ((np.sign(w[1:])) * C) + (-2 * w[1:] * C)\n",
    "        return -gradient\n",
    "    \n",
    "    @staticmethod\n",
    "    def objective_function(w,X,y,C,regularization):\n",
    "        g = expit(X @ w)\n",
    "        # invert this because scipy minimizes, but we derived all formulas for maximzing\n",
    "        return -np.sum(ma.log(g[y==1]))-np.sum(ma.log(1-g[y==0])) + C*sum(w**2) \n",
    "        #-np.sum(y*np.log(g)+(1-y)*np.log(1-g))\n",
    "    \n",
    "    @staticmethod\n",
    "    def BFGS(f, x0, fprime, args, gtol, maxiter):\n",
    "        eta =.1\n",
    "        h = np.identity(len(x0))\n",
    "        hinverse = h\n",
    "        x0 = np.arange(x0.shape[0])\n",
    "        d = len(x0)\n",
    "        g = fprime(x0, args[0], args[1], args[2], args[3])\n",
    "        i = 0\n",
    "        w = x0[:]\n",
    "        while np.linalg.norm(g) > gtol:\n",
    "            if i > maxiter:\n",
    "                break\n",
    "            i += 1\n",
    "            p = -hinverse @ g\n",
    "            w_new = w + eta * p\n",
    "            s = eta * p \n",
    "            \n",
    "            g_new = fprime(w_new, args[0], args[1], args[2], args[3])\n",
    "            v = g_new - g\n",
    "            \n",
    "            u = v - (h @ s)\n",
    "            v = np.array([v])\n",
    "            s = np.array([s])\n",
    "            v = np.reshape(v,(d,1))\n",
    "            s = np.reshape(s,(d,1))\n",
    "            \n",
    "            hnew = h + ((v@v.T)/(v.T@s)) - ((h@s)@(s.T@h))/(s.T@h@s)\n",
    "            hinverse = hinverse + (((s.T@v + hinverse)@(s@s.T))/(s.T@v)**2)-((hinverse@v@s.T + s@v.T@hinverse)/(s.T@v))\n",
    "            print(f\"HNew: {hnew}\")\n",
    "            h = hnew\n",
    "            print(f\"H: {h}\")\n",
    "            w = w_new[:]\n",
    "            g = g_new[:]\n",
    "        return w\n",
    "\n",
    "    # just overwrite fit function\n",
    "    def fit(self, X, y):\n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = self.BFGS(self.objective_function, # what to optimize\n",
    "                            np.zeros((num_features,1)), # starting point\n",
    "                            fprime=self.objective_gradient, # gradient function\n",
    "                            args=(Xb,y,self.C,self.regularization), # extra args for gradient and objective function\n",
    "                            gtol=1e-03, # stopping criteria for gradient, |v_k|\n",
    "                            maxiter=self.iters, # stopping criteria iterations\n",
    "                      )\n",
    "        \n",
    "        self.w_ = self.w_.reshape((num_features,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "47a7b2a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr = MultiClassLogisticRegression(eta=1, iterations=1, C=.001, solver=CustomBFGSBinaryLogisticRegression, regularization='L2')\n",
    "lr1 = MultiClassLogisticRegression(eta=1, iterations=1, C=.001, solver=BFGSBinaryLogisticRegression, regularization='L2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cdd54c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HNew: [[ 9.96117680e-01 -9.07358944e-04 -8.30458457e-04 ... -4.64205088e-03\n",
      "  -4.64234603e-03 -4.70142734e-03]\n",
      " [-9.07358944e-04  9.99788362e-01 -1.93701466e-04 ... -1.08274177e-03\n",
      "  -1.08281061e-03 -1.09659112e-03]\n",
      " [-8.30458457e-04 -1.93701466e-04  9.99822715e-01 ... -9.90977239e-04\n",
      "  -9.91040247e-04 -1.00365283e-03]\n",
      " ...\n",
      " [-4.64205088e-03 -1.08274177e-03 -9.90977239e-04 ...  9.94460690e-01\n",
      "  -5.53966211e-03 -5.61016321e-03]\n",
      " [-4.64234603e-03 -1.08281061e-03 -9.91040247e-04 ... -5.53966211e-03\n",
      "   9.94459986e-01 -5.61051991e-03]\n",
      " [-4.70142734e-03 -1.09659112e-03 -1.00365283e-03 ... -5.61016321e-03\n",
      "  -5.61051991e-03  9.94318077e-01]]\n",
      "H: [[ 9.96117680e-01 -9.07358944e-04 -8.30458457e-04 ... -4.64205088e-03\n",
      "  -4.64234603e-03 -4.70142734e-03]\n",
      " [-9.07358944e-04  9.99788362e-01 -1.93701466e-04 ... -1.08274177e-03\n",
      "  -1.08281061e-03 -1.09659112e-03]\n",
      " [-8.30458457e-04 -1.93701466e-04  9.99822715e-01 ... -9.90977239e-04\n",
      "  -9.91040247e-04 -1.00365283e-03]\n",
      " ...\n",
      " [-4.64205088e-03 -1.08274177e-03 -9.90977239e-04 ...  9.94460690e-01\n",
      "  -5.53966211e-03 -5.61016321e-03]\n",
      " [-4.64234603e-03 -1.08281061e-03 -9.91040247e-04 ... -5.53966211e-03\n",
      "   9.94459986e-01 -5.61051991e-03]\n",
      " [-4.70142734e-03 -1.09659112e-03 -1.00365283e-03 ... -5.61016321e-03\n",
      "  -5.61051991e-03  9.94318077e-01]]\n",
      "HNew: [[ 9.95613984e-01  5.03810647e-04  6.29997368e-04 ... -5.62448100e-03\n",
      "  -5.62496531e-03 -5.72191237e-03]\n",
      " [ 5.03810647e-04  9.95837590e-01 -4.28258144e-03 ...  1.67371145e-03\n",
      "   1.67417254e-03  1.76649872e-03]\n",
      " [ 6.29997368e-04 -4.28258144e-03  9.95590905e-01 ...  1.86156761e-03\n",
      "   1.86205304e-03  1.95925208e-03]\n",
      " ...\n",
      " [-5.62448100e-03  1.67371145e-03  1.86156761e-03 ...  9.92550371e-01\n",
      "  -7.45034977e-03 -7.59467552e-03]\n",
      " [-5.62496531e-03  1.67417254e-03  1.86205304e-03 ... -7.45034977e-03\n",
      "   9.92548929e-01 -7.59541554e-03]\n",
      " [-5.72191237e-03  1.76649872e-03  1.95925208e-03 ... -7.59467552e-03\n",
      "  -7.59541554e-03  9.92256496e-01]]\n",
      "H: [[ 9.95613984e-01  5.03810647e-04  6.29997368e-04 ... -5.62448100e-03\n",
      "  -5.62496531e-03 -5.72191237e-03]\n",
      " [ 5.03810647e-04  9.95837590e-01 -4.28258144e-03 ...  1.67371145e-03\n",
      "   1.67417254e-03  1.76649872e-03]\n",
      " [ 6.29997368e-04 -4.28258144e-03  9.95590905e-01 ...  1.86156761e-03\n",
      "   1.86205304e-03  1.95925208e-03]\n",
      " ...\n",
      " [-5.62448100e-03  1.67371145e-03  1.86156761e-03 ...  9.92550371e-01\n",
      "  -7.45034977e-03 -7.59467552e-03]\n",
      " [-5.62496531e-03  1.67417254e-03  1.86205304e-03 ... -7.45034977e-03\n",
      "   9.92548929e-01 -7.59541554e-03]\n",
      " [-5.72191237e-03  1.76649872e-03  1.95925208e-03 ... -7.59467552e-03\n",
      "  -7.59541554e-03  9.92256496e-01]]\n",
      "HNew: [[ 9.98513160e-01 -3.45427681e-04 -2.94481103e-04 ... -2.87994730e-03\n",
      "  -2.88466528e-03 -2.90752161e-03]\n",
      " [-3.45427681e-04  9.99919910e-01 -6.82778046e-05 ... -6.67738870e-04\n",
      "  -6.68832772e-04 -6.74132194e-04]\n",
      " [-2.94481103e-04 -6.82778046e-05  9.99941792e-01 ... -5.69255129e-04\n",
      "  -5.70187693e-04 -5.74705512e-04]\n",
      " ...\n",
      " [-2.87994730e-03 -6.67738870e-04 -5.69255129e-04 ...  9.94432835e-01\n",
      "  -5.57628483e-03 -5.62046790e-03]\n",
      " [-2.88466528e-03 -6.68832772e-04 -5.70187693e-04 ... -5.57628483e-03\n",
      "   9.94414580e-01 -5.62967546e-03]\n",
      " [-2.90752161e-03 -6.74132194e-04 -5.74705512e-04 ... -5.62046790e-03\n",
      "  -5.62967546e-03  9.94325718e-01]]\n",
      "H: [[ 9.98513160e-01 -3.45427681e-04 -2.94481103e-04 ... -2.87994730e-03\n",
      "  -2.88466528e-03 -2.90752161e-03]\n",
      " [-3.45427681e-04  9.99919910e-01 -6.82778046e-05 ... -6.67738870e-04\n",
      "  -6.68832772e-04 -6.74132194e-04]\n",
      " [-2.94481103e-04 -6.82778046e-05  9.99941792e-01 ... -5.69255129e-04\n",
      "  -5.70187693e-04 -5.74705512e-04]\n",
      " ...\n",
      " [-2.87994730e-03 -6.67738870e-04 -5.69255129e-04 ...  9.94432835e-01\n",
      "  -5.57628483e-03 -5.62046790e-03]\n",
      " [-2.88466528e-03 -6.68832772e-04 -5.70187693e-04 ... -5.57628483e-03\n",
      "   9.94414580e-01 -5.62967546e-03]\n",
      " [-2.90752161e-03 -6.74132194e-04 -5.74705512e-04 ... -5.62046790e-03\n",
      "  -5.62967546e-03  9.94325718e-01]]\n",
      "HNew: [[ 0.99811448 -0.00178758 -0.00178302 ... -0.00201466 -0.00201509\n",
      "  -0.00201713]\n",
      " [-0.00178758  0.99470484 -0.00545116 ...  0.00246562  0.00248007\n",
      "   0.00255006]\n",
      " [-0.00178302 -0.00545116  0.9943857  ...  0.0026647   0.00267981\n",
      "   0.002753  ]\n",
      " ...\n",
      " [-0.00201466  0.00246562  0.0026647  ...  0.99256169 -0.00745674\n",
      "  -0.00754606]\n",
      " [-0.00201509  0.00248007  0.00267981 ... -0.00745674  0.99252476\n",
      "  -0.00756485]\n",
      " [-0.00201713  0.00255006  0.002753   ... -0.00754606 -0.00756485\n",
      "   0.9923441 ]]\n",
      "H: [[ 0.99811448 -0.00178758 -0.00178302 ... -0.00201466 -0.00201509\n",
      "  -0.00201713]\n",
      " [-0.00178758  0.99470484 -0.00545116 ...  0.00246562  0.00248007\n",
      "   0.00255006]\n",
      " [-0.00178302 -0.00545116  0.9943857  ...  0.0026647   0.00267981\n",
      "   0.002753  ]\n",
      " ...\n",
      " [-0.00201466  0.00246562  0.0026647  ...  0.99256169 -0.00745674\n",
      "  -0.00754606]\n",
      " [-0.00201509  0.00248007  0.00267981 ... -0.00745674  0.99252476\n",
      "  -0.00756485]\n",
      " [-0.00201713  0.00255006  0.002753   ... -0.00754606 -0.00756485\n",
      "   0.9923441 ]]\n",
      "HNew: [[ 9.98333094e-01 -3.02607943e-04 -3.90136302e-04 ... -3.05295239e-03\n",
      "  -3.05394666e-03 -3.07974450e-03]\n",
      " [-3.02607943e-04  9.99945175e-01 -7.06829517e-05 ... -5.53118706e-04\n",
      "  -5.53298844e-04 -5.57972767e-04]\n",
      " [-3.90136302e-04 -7.06829517e-05  9.99908872e-01 ... -7.13106483e-04\n",
      "  -7.13338725e-04 -7.19364566e-04]\n",
      " ...\n",
      " [-3.05295239e-03 -5.53118706e-04 -7.13106483e-04 ...  9.94419694e-01\n",
      "  -5.58212388e-03 -5.62927818e-03]\n",
      " [-3.05394666e-03 -5.53298844e-04 -7.13338725e-04 ... -5.58212388e-03\n",
      "   9.94416058e-01 -5.63111150e-03]\n",
      " [-3.07974450e-03 -5.57972767e-04 -7.19364566e-04 ... -5.62927818e-03\n",
      "  -5.63111150e-03  9.94321320e-01]]\n",
      "H: [[ 9.98333094e-01 -3.02607943e-04 -3.90136302e-04 ... -3.05295239e-03\n",
      "  -3.05394666e-03 -3.07974450e-03]\n",
      " [-3.02607943e-04  9.99945175e-01 -7.06829517e-05 ... -5.53118706e-04\n",
      "  -5.53298844e-04 -5.57972767e-04]\n",
      " [-3.90136302e-04 -7.06829517e-05  9.99908872e-01 ... -7.13106483e-04\n",
      "  -7.13338725e-04 -7.19364566e-04]\n",
      " ...\n",
      " [-3.05295239e-03 -5.53118706e-04 -7.13106483e-04 ...  9.94419694e-01\n",
      "  -5.58212388e-03 -5.62927818e-03]\n",
      " [-3.05394666e-03 -5.53298844e-04 -7.13338725e-04 ... -5.58212388e-03\n",
      "   9.94416058e-01 -5.63111150e-03]\n",
      " [-3.07974450e-03 -5.57972767e-04 -7.19364566e-04 ... -5.62927818e-03\n",
      "  -5.63111150e-03  9.94321320e-01]]\n",
      "HNew: [[ 0.99807844 -0.00149991 -0.00152719 ... -0.00235729 -0.0023576\n",
      "  -0.00236564]\n",
      " [-0.00149991  0.99431732 -0.00541524 ...  0.00272086  0.0027239\n",
      "   0.00280272]\n",
      " [-0.00152719 -0.00541524  0.99483336 ...  0.00239642  0.00239925\n",
      "   0.00247252]\n",
      " ...\n",
      " [-0.00235729  0.00272086  0.00239642 ...  0.99252627 -0.00747742\n",
      "  -0.00757304]\n",
      " [-0.0023576   0.0027239   0.00239925 ... -0.00747742  0.99251889\n",
      "  -0.00757679]\n",
      " [-0.00236564  0.00280272  0.00247252 ... -0.00757304 -0.00757679\n",
      "   0.99232588]]\n",
      "H: [[ 0.99807844 -0.00149991 -0.00152719 ... -0.00235729 -0.0023576\n",
      "  -0.00236564]\n",
      " [-0.00149991  0.99431732 -0.00541524 ...  0.00272086  0.0027239\n",
      "   0.00280272]\n",
      " [-0.00152719 -0.00541524  0.99483336 ...  0.00239642  0.00239925\n",
      "   0.00247252]\n",
      " ...\n",
      " [-0.00235729  0.00272086  0.00239642 ...  0.99252627 -0.00747742\n",
      "  -0.00757304]\n",
      " [-0.0023576   0.0027239   0.00239925 ... -0.00747742  0.99251889\n",
      "  -0.00757679]\n",
      " [-0.00236564  0.00280272  0.00247252 ... -0.00757304 -0.00757679\n",
      "   0.99232588]]\n"
     ]
    }
   ],
   "source": [
    "lr1.fit(X_train, y_train)\n",
    "lr.fit(X_train, y_train)\n",
    "yhat = lr.predict(X_test)\n",
    "acc = accuracy_score(y_test,yhat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
